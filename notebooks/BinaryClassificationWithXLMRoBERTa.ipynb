{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"from pathlib import Path\nimport os\n\nDATA_PATH = Path(\"/kaggle/input/interiitahsg/\")\nCURRENT_PATH = Path(\"/kaggle/working/\")\n\nos.makedirs(\"models\", exist_ok=True)","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd \n\nimport transformers\nimport torch\nimport pytorch_lightning as pl\n\nfrom argparse import ArgumentParser\n\nclass BinaryClassificationDataset(torch.utils.data.Dataset):\n    def __init__(\n        self,\n        args,\n        main_text,\n        mobile_tech_label,\n    ):\n        self.hparams = args\n        self.main_text = main_text\n        self.mobile_tech_label = mobile_tech_label\n        \n        self.tokenizer = transformers.AutoTokenizer.from_pretrained(args.base_path)\n\n    def __len__(self):\n        return len(self.mobile_tech_label)\n\n    def __getitem__(self, idx):\n        # print(\"idx: \", idx)\n        main_text = self.main_text[idx]\n        mobile_tech_label = self.mobile_tech_label[idx]\n\n        inputs = self.tokenizer(\n            main_text,\n            add_special_tokens=True,\n            padding=\"max_length\",\n            truncation=True,\n            max_length=self.hparams.maxlen,\n        )\n        return {\n            \"ids_seq\": torch.tensor(inputs[\"input_ids\"], dtype=torch.long),\n            \"attn_masks\": torch.tensor(inputs[\"attention_mask\"], dtype=torch.long),\n            \"target\": torch.tensor(mobile_tech_label, dtype=torch.float),\n        }\n        \nclass BinaryClassificationDataModule(pl.LightningDataModule):\n    \n    def __init__(self, args):\n        super().__init__()\n        self.hparams = args\n        \n    def train_dataloader(self):\n        article = pd.read_pickle(DATA_PATH/\"data/article_train_cleaned.pkl\")\n        tweet = pd.read_pickle(DATA_PATH/\"data/tweet_train_cleaned.pkl\")\n        \n        article = article.loc[:,[\"Text\", \"Mobile_Tech_Flag\"]]\n        tweet = tweet.loc[:,[\"Tweet_with_emoji_desc\", \"Mobile_Tech_Tag\"]].rename(columns={\"Tweet_with_emoji_desc\":\"Text\", \"Mobile_Tech_Tag\":\"Mobile_Tech_Flag\"})\n\n        combined = pd.concat([article, tweet]).sample(frac=1.0)\n\n        ds = BinaryClassificationDataset(self.hparams, combined.Text.to_list(), combined.Mobile_Tech_Flag.to_list())\n\n        return torch.utils.data.DataLoader(\n            ds,\n            batch_size=self.hparams.batch_size,\n            shuffle=True,\n            num_workers=8,\n            drop_last=True,\n        )\n    \n    def val_dataloader(self):\n        article = pd.read_pickle(DATA_PATH/\"data/article_dev_cleaned.pkl\")\n        tweet = pd.read_pickle(DATA_PATH/\"data/tweet_dev_cleaned.pkl\")\n        \n        article = article.loc[:,[\"Text\", \"Mobile_Tech_Flag\"]]\n        tweet = tweet.loc[:,[\"Tweet_with_emoji_desc\", \"Mobile_Tech_Tag\"]].rename(columns={\"Tweet_with_emoji_desc\":\"Text\", \"Mobile_Tech_Tag\":\"Mobile_Tech_Flag\"})\n\n        combined = pd.concat([article, tweet])\n\n        ds = BinaryClassificationDataset(self.hparams, combined.Text.to_list(), combined.Mobile_Tech_Flag.to_list())\n\n        return torch.utils.data.DataLoader(\n            ds,\n            batch_size=self.hparams.batch_size*2,\n            shuffle=False,\n            num_workers=8,\n            drop_last=False,\n        )","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn import metrics\nimport torch\nimport torch.nn as nn\nimport pytorch_lightning as pl\n\nimport transformers\nfrom transformers import AdamW, get_cosine_schedule_with_warmup\n\n\nclass MainModel(nn.Module):\n    def __init__(self, args=None, **kwargs):\n        super().__init__()\n        self.base = transformers.AutoModel.from_pretrained(args.base_path)\n        self.dropout = nn.Dropout(0.3)\n        self.linear = nn.Linear(768,1)\n\n    def forward(self, ids_seq, attn_masks, token_type_ids=None):\n        base_out = self.base(\n            ids_seq, attention_mask=attn_masks, token_type_ids=token_type_ids\n        )\n        # using maxpooled output\n        max_out = self.dropout(base_out[1])\n        return self.linear(max_out)\n\n\nclass SequenceClassicationLightningModule(pl.LightningModule):\n    def __init__(self, args, **kwargs):\n        super().__init__()\n\n        self.save_hyperparameters(args)\n        self.model = MainModel(self.hparams)\n\n    @staticmethod\n    def loss(logits, targets):\n        return nn.BCEWithLogitsLoss()(logits, targets)\n\n    def shared_step(self, batch):\n        ids_seq, attn_masks, target = (\n            batch[\"ids_seq\"],\n            batch[\"attn_masks\"],\n            batch[\"target\"],\n        )\n        logits = self.model(ids_seq, attn_masks).squeeze()\n        loss = self.loss(logits, target)\n        return logits, loss\n\n    def training_step(self, batch, batch_idx):\n        logits, loss = self.shared_step(batch)\n\n        self.log(\n            \"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True\n        )\n\n        return {\"loss\": loss, \"logits\": logits, \"true_preds\": batch[\"target\"]}\n\n    def validation_step(self, batch, batch_idx):\n        logits, loss = self.shared_step(batch)\n\n        self.log(\n            \"valid_loss\",\n            loss,\n            on_step=False,\n            on_epoch=True,\n            prog_bar=True,\n            logger=True,\n        )\n        return {\"logits\": logits, \"true_preds\": batch[\"target\"]}\n\n    def configure_optimizers(self):\n        grouped_parameters = [\n            {\"params\": self.model.base.parameters(), \"lr\": self.hparams.base_lr},\n            {\"params\": self.model.linear.parameters(), \"lr\": self.hparams.linear_lr},\n        ]\n        optim = AdamW(grouped_parameters, lr=self.hparams.base_lr)\n\n        # num_training_steps = (\n        #     4863 // (self.hparams.batch_size * self.hparams.accumulate_grad_batches)\n        # ) * self.hparams.max_epochs\n        # sched = get_cosine_schedule_with_warmup(\n        #     optim, num_warmup_steps=0, num_training_steps=num_training_steps\n        # )\n\n        # return [optim], [sched]\n        return optim\n\n    def training_epoch_end(self, training_step_outputs):\n        y_pred = torch.sigmoid(torch.cat([out[\"logits\"] for out in training_step_outputs])).to(\"cpu\").detach().numpy() >= 0.5\n        y_true = torch.cat([out[\"true_preds\"] for out in training_step_outputs]).to(\"cpu\", dtype=int).detach().numpy()\n        \n        acc = metrics.accuracy_score(y_pred, y_true)\n        f1 = metrics.f1_score(y_pred, y_true)\n        \n        self.log(\"train_acc\", acc)\n        self.log(\"train_f1\", f1)\n    \n    def validation_epoch_end(self, validation_step_outputs):\n        y_pred = torch.sigmoid(torch.cat([out[\"logits\"] for out in validation_step_outputs])).to(\"cpu\").detach().numpy() >= 0.5\n        y_true = torch.cat([out[\"true_preds\"] for out in validation_step_outputs]).to(\"cpu\", dtype=int).detach().numpy()\n        \n        acc = metrics.accuracy_score(y_pred, y_true)\n        f1 = metrics.f1_score(y_pred, y_true)\n        \n        self.log(\"val_acc\", acc)\n        self.log(\"val_f1\", f1)","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import numpy as np\n# from sklearn import metrics\n\n# ypred = np.random.uniform(0,1,size=(128,))\n# ytrue = np.random.randint(0,2,size=(128,))\n\n# ypred = ypred >=0.5\n# print(metrics.accuracy_score(ypred,ytrue))","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nfrom argparse import ArgumentParser\n\nimport torch\nimport pytorch_lightning as pl\n\nimport wandb\nfrom pytorch_lightning.loggers import WandbLogger\n\nfrom pytorch_lightning.callbacks.early_stopping import EarlyStopping\n\n# from dataloader import BinaryClassificationDataModule\n# from model import SequenceClassicationLightningModule\n\nclass ToggleBaseTraining(pl.Callback):\n    def on_train_epoch_start(self, trainer, pl_module):\n        print(\"-\" * 100)\n        print(\"ToggleBaseTraining Callback working.............\")\n        if trainer.current_epoch == 0:\n            print(\n                f\"current_epoch is: {trainer.current_epoch} and freezing BERT layer's parameters\"\n            )\n            for p in pl_module.model.base.parameters():\n                p.requires_grad = False\n        else:\n            print(\n                f\"current_epoch is: {trainer.current_epoch} and unfreezing BERT layer's parameters for training\"\n            )\n            for p in pl_module.model.base.parameters():\n                p.requires_grad = True\n        print(\"-\" * 100)\n\nclass SaveModelWeights(pl.Callback):\n    def __init__(self, save_from_epoch=1):\n        self.save_from_epoch =save_from_epoch\n\n    def on_validation_end(self, trainer, pl_module):\n        os.makedirs(\"../models/\", exist_ok=True)\n        print(\"-\" * 100)\n        print(\"SaveModelWeight Callback working.............\")\n        print(f\"trainer.current_epoch: {trainer.current_epoch}\")\n        if trainer.current_epoch >= self.save_from_epoch:\n            m_filepath = f\"../models/{pl_module.hparams.model_name}-epoch-{trainer.current_epoch}.pt\"\n            torch.save(pl_module.model.state_dict(), m_filepath)\n            print(f\"saved current model weights in file: {m_filepath}\")\n        print(\"-\" * 100)","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pl.seed_everything(420)\n\nparser = ArgumentParser()\n\n# trainer related arguments\nparser.add_argument(\n    \"--gpus\",\n    default=1,\n    help=\"if value is 0 cpu will be used, if string then that gpu device will be used\",\n)\nparser.add_argument(\"--checkpoint_callback\", action=\"store_true\")\nparser.add_argument(\"--logger\", action=\"store_true\")\nparser.add_argument(\"--max_epochs\", default=5, type=int)\nparser.add_argument(\"--progress_bar_refresh_rate\", default=0, type=int)\nparser.add_argument(\"--accumulate_grad_batches\", default=2, type=int)\nparser.add_argument(\"--model_name\", default=\"ahsg\", type=str)\n\n# data related arguments\nparser.add_argument(\"--batch_size\", default=8, type=int)\nparser.add_argument(\"--maxlen\", default=512, type=int)\n\n# model related arguments\nparser.add_argument(\"--base_path\", type=str, default=\"xlm-roberta-base\")\nparser.add_argument(\"--base_lr\", default=1e-5, type=int)\nparser.add_argument(\"--linear_lr\", default=5e-3, type=int)\nparser.add_argument(\"--base_dropout\", default=0.3, type=float)\nparser.add_argument(\n    \"--bert_output_used\",\n    default=\"maxpooled\",\n    type=str,\n    choices=[\"maxpooled\", \"weighted_sum\"],\n)\nparser.add_argument(\"--run_name\", default=None)\n# parser = pl.Trainer.add_argparse_args(parser)\n\nargs = parser.parse_known_args()\nargs = args[0]\n\nargs.effective_batch_size = args.batch_size * args.accumulate_grad_batches\nargs.log_every_n_steps = args.accumulate_grad_batches * 5\n\nif not torch.cuda.is_available():\n    args.gpus = 0","execution_count":6,"outputs":[{"output_type":"stream","text":"Global seed set to 420\n","name":"stderr"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"args.gpus = 1\nargs.accumulate_grad_batches = 1\nargs.batch_size = 8\nargs.run_name = \"test6\"\nargs.logger = True","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pl_model = SequenceClassicationLightningModule(args)\ndata = BinaryClassificationDataModule(args)","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# wandb.login()","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if args.logger:\n    args.logger = WandbLogger(\n        project=\"ahsg\", entity='professor',\n        name=args.run_name if (args.run_name is not None) else None,\n    )\n\ntrainer = pl.Trainer.from_argparse_args(\n    args,\n    callbacks=[\n        ToggleBaseTraining(),\n        SaveModelWeights(save_from_epoch=0),\n    ],\n)\n\nprint(\nf\"Training model_name={args.model_name} for epochs={args.max_epochs} with an effective_batch_size={args.effective_batch_size}\"\n)","execution_count":10,"outputs":[{"output_type":"stream","text":"GPU available: True, used: True\nTPU available: None, using: 0 TPU cores\n","name":"stderr"},{"output_type":"stream","text":"Training model_name=ahsg for epochs=5 with an effective_batch_size=16\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"trainer.fit(pl_model, data)","execution_count":11,"outputs":[{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","name":"stderr"},{"output_type":"stream","name":"stdout","text":"wandb: Paste an API key from your profile and hit enter: ········\n"},{"output_type":"stream","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.10.21 is available!  To upgrade, please run:\n\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n","name":"stderr"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n                Tracking run with wandb version 0.10.19<br/>\n                Syncing run <strong style=\"color:#cdcd00\">test6</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/professor/ahsg\" target=\"_blank\">https://wandb.ai/professor/ahsg</a><br/>\n                Run page: <a href=\"https://wandb.ai/professor/ahsg/runs/2pzsxeqw\" target=\"_blank\">https://wandb.ai/professor/ahsg/runs/2pzsxeqw</a><br/>\n                Run data is saved locally in <code>/kaggle/working/wandb/run-20210308_231245-2pzsxeqw</code><br/><br/>\n            "},"metadata":{}},{"output_type":"stream","text":"\n  | Name  | Type      | Params\n------------------------------------\n0 | model | MainModel | 278 M \n------------------------------------\n278 M     Trainable params\n0         Non-trainable params\n278 M     Total params\n1,112.178 Total estimated model params size (MB)\n","name":"stderr"},{"output_type":"stream","text":"----------------------------------------------------------------------------------------------------\nSaveModelWeight Callback working.............\ntrainer.current_epoch: 0\nsaved current model weights in file: ../models/ahsg-epoch-0.pt\n----------------------------------------------------------------------------------------------------\n----------------------------------------------------------------------------------------------------\nToggleBaseTraining Callback working.............\ncurrent_epoch is: 0 and freezing BERT layer's parameters\n----------------------------------------------------------------------------------------------------\n----------------------------------------------------------------------------------------------------\nSaveModelWeight Callback working.............\ntrainer.current_epoch: 0\nsaved current model weights in file: ../models/ahsg-epoch-0.pt\n----------------------------------------------------------------------------------------------------\n----------------------------------------------------------------------------------------------------\nToggleBaseTraining Callback working.............\ncurrent_epoch is: 1 and unfreezing BERT layer's parameters for training\n----------------------------------------------------------------------------------------------------\n----------------------------------------------------------------------------------------------------\nSaveModelWeight Callback working.............\ntrainer.current_epoch: 1\nsaved current model weights in file: ../models/ahsg-epoch-1.pt\n----------------------------------------------------------------------------------------------------\n----------------------------------------------------------------------------------------------------\nToggleBaseTraining Callback working.............\ncurrent_epoch is: 2 and unfreezing BERT layer's parameters for training\n----------------------------------------------------------------------------------------------------\n----------------------------------------------------------------------------------------------------\nSaveModelWeight Callback working.............\ntrainer.current_epoch: 2\nsaved current model weights in file: ../models/ahsg-epoch-2.pt\n----------------------------------------------------------------------------------------------------\n----------------------------------------------------------------------------------------------------\nToggleBaseTraining Callback working.............\ncurrent_epoch is: 3 and unfreezing BERT layer's parameters for training\n----------------------------------------------------------------------------------------------------\n----------------------------------------------------------------------------------------------------\nSaveModelWeight Callback working.............\ntrainer.current_epoch: 3\nsaved current model weights in file: ../models/ahsg-epoch-3.pt\n----------------------------------------------------------------------------------------------------\n----------------------------------------------------------------------------------------------------\nToggleBaseTraining Callback working.............\ncurrent_epoch is: 4 and unfreezing BERT layer's parameters for training\n----------------------------------------------------------------------------------------------------\n----------------------------------------------------------------------------------------------------\nSaveModelWeight Callback working.............\ntrainer.current_epoch: 4\nsaved current model weights in file: ../models/ahsg-epoch-4.pt\n----------------------------------------------------------------------------------------------------\n","name":"stdout"},{"output_type":"execute_result","execution_count":11,"data":{"text/plain":"1"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"wandb.finish()","execution_count":13,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br/>Waiting for W&B process to finish, PID 452<br/>Program ended successfully."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value=' 0.00MB of 0.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find user logs for this run at: <code>/kaggle/working/wandb/run-20210308_231245-2pzsxeqw/logs/debug.log</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find internal logs for this run at: <code>/kaggle/working/wandb/run-20210308_231245-2pzsxeqw/logs/debug-internal.log</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<h3>Run summary:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>train_loss_step</td><td>0.00045</td></tr><tr><td>epoch</td><td>4</td></tr><tr><td>_runtime</td><td>1737</td></tr><tr><td>_timestamp</td><td>1615246902</td></tr><tr><td>_step</td><td>3864</td></tr><tr><td>train_loss_epoch</td><td>0.06461</td></tr><tr><td>train_acc</td><td>0.9806</td></tr><tr><td>train_f1</td><td>0.96068</td></tr><tr><td>valid_loss</td><td>0.0838</td></tr><tr><td>val_acc</td><td>0.97822</td></tr><tr><td>val_f1</td><td>0.95582</td></tr></table>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<h3>Run history:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>train_loss_step</td><td>▇▆▇▅▆▆█▆▅▁▁▁▃▃▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▂</td></tr><tr><td>epoch</td><td>▁▁▁▁▁▁▁▁▃▃▃▃▃▃▃▃▅▅▅▅▅▅▅▅▆▆▆▆▆▆▆▆████████</td></tr><tr><td>_runtime</td><td>▁▁▁▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_timestamp</td><td>▁▁▁▁▁▁▁▁▂▂▂▂▂▃▃▃▃▃▄▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>_step</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>train_loss_epoch</td><td>█▂▁▁▁</td></tr><tr><td>train_acc</td><td>▁▇███</td></tr><tr><td>train_f1</td><td>▁████</td></tr><tr><td>valid_loss</td><td>█▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁████</td></tr><tr><td>val_f1</td><td>▁████</td></tr></table><br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n                    <br/>Synced <strong style=\"color:#cdcd00\">test6</strong>: <a href=\"https://wandb.ai/professor/ahsg/runs/2pzsxeqw\" target=\"_blank\">https://wandb.ai/professor/ahsg/runs/2pzsxeqw</a><br/>\n                "},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}